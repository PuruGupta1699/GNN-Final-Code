Epoch 1/10
5/5 [==============================] - 1s 2ms/step - loss: 1479.1875
Epoch 2/10
5/5 [==============================] - 0s 2ms/step - loss: 1141.9849
Epoch 3/10
5/5 [==============================] - 0s 2ms/step - loss: 967.7220
Epoch 4/10
5/5 [==============================] - 0s 2ms/step - loss: 782.3619
Epoch 5/10
5/5 [==============================] - 0s 1ms/step - loss: 679.6906
Epoch 6/10
5/5 [==============================] - 0s 1ms/step - loss: 591.5679
Epoch 7/10
5/5 [==============================] - 0s 1ms/step - loss: 540.6999
Epoch 8/10
5/5 [==============================] - 0s 1ms/step - loss: 502.5316
Epoch 9/10
5/5 [==============================] - 0s 1ms/step - loss: 488.4699
Epoch 10/10
5/5 [==============================] - 0s 999us/step - loss: 445.2430
Epoch 1/10
11/11 [==============================] - 0s 1ms/step - loss: 847.3836
Epoch 2/10
11/11 [==============================] - 0s 2ms/step - loss: 636.0625
Epoch 3/10
11/11 [==============================] - 0s 1ms/step - loss: 547.8194
Epoch 4/10
11/11 [==============================] - 0s 1ms/step - loss: 473.9043
Epoch 5/10
11/11 [==============================] - 0s 1ms/step - loss: 436.2554
Epoch 6/10
11/11 [==============================] - 0s 1ms/step - loss: 420.2030
Epoch 7/10
11/11 [==============================] - 0s 1ms/step - loss: 403.9496
Epoch 8/10
11/11 [==============================] - 0s 1ms/step - loss: 394.7898
Epoch 9/10
11/11 [==============================] - 0s 1ms/step - loss: 393.6453
Epoch 10/10
11/11 [==============================] - 0s 3ms/step - loss: 391.9339
Epoch 1/10
13/13 [==============================] - 0s 1ms/step - loss: 677.3236
Epoch 2/10
13/13 [==============================] - 0s 2ms/step - loss: 519.3354
Epoch 3/10
13/13 [==============================] - 0s 2ms/step - loss: 445.9961
Epoch 4/10
13/13 [==============================] - 0s 1ms/step - loss: 396.7707
Epoch 5/10
13/13 [==============================] - 0s 1ms/step - loss: 372.9503
Epoch 6/10
13/13 [==============================] - 0s 1ms/step - loss: 351.9297
Epoch 7/10
13/13 [==============================] - 0s 1ms/step - loss: 334.1411
Epoch 8/10
13/13 [==============================] - 0s 1ms/step - loss: 334.5717
Epoch 9/10
13/13 [==============================] - 0s 1ms/step - loss: 337.6306
Epoch 10/10
13/13 [==============================] - 0s 2ms/step - loss: 321.0551
Epoch 1/10
7/7 [==============================] - 0s 1ms/step - loss: 1454.6708
Epoch 2/10
7/7 [==============================] - 0s 2ms/step - loss: 1119.1057
Epoch 3/10
7/7 [==============================] - 0s 2ms/step - loss: 918.3848
Epoch 4/10
7/7 [==============================] - 0s 1ms/step - loss: 754.4626
Epoch 5/10
7/7 [==============================] - 0s 1ms/step - loss: 662.7352
Epoch 6/10
7/7 [==============================] - 0s 2ms/step - loss: 596.4769
Epoch 7/10
7/7 [==============================] - 0s 2ms/step - loss: 553.6462
Epoch 8/10
7/7 [==============================] - 0s 1ms/step - loss: 531.4553
Epoch 9/10
7/7 [==============================] - 0s 1ms/step - loss: 521.9948
Epoch 10/10
7/7 [==============================] - 0s 1ms/step - loss: 517.8166
Epoch 1/10
9/9 [==============================] - 0s 1ms/step - loss: 927.2278
Epoch 2/10
9/9 [==============================] - 0s 1ms/step - loss: 743.0984
Epoch 3/10
9/9 [==============================] - 0s 2ms/step - loss: 647.6102
Epoch 4/10
9/9 [==============================] - 0s 1ms/step - loss: 567.0743
Epoch 5/10
9/9 [==============================] - 0s 1ms/step - loss: 521.4862
Epoch 6/10
9/9 [==============================] - 0s 1ms/step - loss: 497.6510
Epoch 7/10
9/9 [==============================] - 0s 1ms/step - loss: 477.7352
Epoch 8/10
9/9 [==============================] - 0s 1ms/step - loss: 471.8416
Epoch 9/10
9/9 [==============================] - 0s 1ms/step - loss: 468.0903
Epoch 10/10
9/9 [==============================] - 0s 1ms/step - loss: 466.3930
Epoch 1/10
13/13 [==============================] - 0s 1ms/step - loss: 975.3398
Epoch 2/10
13/13 [==============================] - 0s 2ms/step - loss: 722.5175
Epoch 3/10
13/13 [==============================] - 0s 2ms/step - loss: 600.2354
Epoch 4/10
13/13 [==============================] - 0s 1ms/step - loss: 516.0691
Epoch 5/10
13/13 [==============================] - 0s 1ms/step - loss: 447.6021
Epoch 6/10
13/13 [==============================] - 0s 1ms/step - loss: 430.2722
Epoch 7/10
13/13 [==============================] - 0s 997us/step - loss: 399.8737
Epoch 8/10
13/13 [==============================] - 0s 1ms/step - loss: 393.9015
Epoch 9/10
13/13 [==============================] - 0s 1ms/step - loss: 386.8727
Epoch 10/10
13/13 [==============================] - 0s 1ms/step - loss: 375.1393
C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\trainer.py:138: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:210.)
  input_batch_org=torch.Tensor(input_batch_org).to(self.device)
Traceback (most recent call last):
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\main.py", line 81, in <module>
    main()
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\main.py", line 43, in main
    trainer.run()
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\trainer.py", line 123, in run
    input_batch,input_batch_org = self.dataset.train_batch(
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\data_generator_v2.py", line 268, in train_batch
    A=self.getEmbeddings(num_walks,walk_max_length,G,embedding_dim,num_epochs,prob_p,prob_q,num_negative_samples)
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\data_generator_v2.py", line 217, in getEmbeddings
    targets, contexts, labels, weights = self.generate_examples(
  File "C:\Users\tulsy\Desktop\puru\NOC-Mapping-Problem\data_generator_v2.py", line 153, in generate_examples
    pairs, labels = keras.preprocessing.sequence.skipgrams(
  File "C:\Users\tulsy\AppData\Local\Programs\Python\Python39\lib\site-packages\keras_preprocessing\sequence.py", line 224, in skipgrams
    couples += [[words[i % len(words)],
  File "C:\Users\tulsy\AppData\Local\Programs\Python\Python39\lib\site-packages\keras_preprocessing\sequence.py", line 225, in <listcomp>
    random.randint(1, vocabulary_size - 1)]
  File "C:\Users\tulsy\AppData\Local\Programs\Python\Python39\lib\random.py", line 338, in randint
    return self.randrange(a, b+1)
  File "C:\Users\tulsy\AppData\Local\Programs\Python\Python39\lib\random.py", line 314, in randrange
    return istart + self._randbelow(width)
KeyboardInterrupt